from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import numpy as np
import onnxruntime as ort
from transformers import AlbertTokenizer
import threading

# =========================================================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# =========================================================
app = FastAPI(title="Improved Arabic ALBERT Embedding API ğŸš€")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================================================
# Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù…Ø© (lazy loading)
# =========================================================
MODEL_PATH = "models/albert_arabic_wa_merged.onnx"
TOKENIZER_PATH = "models/asafaya/albert-base-arabic"

TARGET_DIM_DEFAULT = 384
embedding_dim_original = 768

tokenizer = None
session = None
projection_matrix = None
model_lock = threading.Lock()

# =========================================================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨ ÙÙ‚Ø· (Lazy Load)
# =========================================================
def load_model():
    global tokenizer, session, projection_matrix

    with model_lock:
        if tokenizer is not None and session is not None:
            return

        print("ğŸ“¦ ØªØ­Ù…ÙŠÙ„ tokenizer ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬ ...")
        tokenizer = AlbertTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)
        session = ort.InferenceSession(
            MODEL_PATH,
            providers=["CPUExecutionProvider"],
        )

        print("ğŸ“Š ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· (PCA) Ù…Ø­Ø³Ù‘Ù†Ø© ...")
        # Ù…Ø­Ø§ÙƒØ§Ø© Ù…ØµÙÙˆÙØ© PCA Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ (ÙÙŠ Ù…Ø´Ø±ÙˆØ¹ Ø­Ù‚ÙŠÙ‚ÙŠ ÙŠØ¬Ø¨ Ø­Ø³Ø§Ø¨Ù‡Ø§ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©)
        np.random.seed(42)
        pca_matrix = np.random.randn(embedding_dim_original, TARGET_DIM_DEFAULT).astype(np.float32)
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø«Ø¨Ø§Øª Ø§Ù„Ø¹Ø¯Ø¯ÙŠ
        pca_matrix /= np.linalg.norm(pca_matrix, axis=0, keepdims=True)
        projection_matrix = pca_matrix

        print("âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù….")

# =========================================================
# Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
# =========================================================
class TextInput(BaseModel):
    text: str
    normalize: bool = True
    reduce_dim: int = TARGET_DIM_DEFAULT
    pooling: str = "mean"  # "mean" Ø£Ùˆ "cls"

# =========================================================
# Health Check
# =========================================================
@app.get("/health")
def health_check():
    return {"status": "ok", "model_loaded": session is not None}

# =========================================================
# Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# =========================================================
@app.get("/")
def home():
    return {"message": "Improved Arabic ALBERT Embedding API is running ğŸš€"}

# =========================================================
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ embedding
# =========================================================
@app.post("/embed")
def get_embedding(input: TextInput):
    if not input.text.strip():
        raise HTTPException(status_code=400, detail="Ø§Ù„Ù†Øµ ÙØ§Ø±Øº")

    if tokenizer is None or session is None:
        load_model()

    # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ
    inputs = tokenizer(
        input.text,
        return_tensors="np",
        truncation=True,
        max_length=128,
        padding="max_length",
    )

    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    outputs = session.run(None, {"input_ids": input_ids, "attention_mask": attention_mask})
    last_hidden_state = outputs[0]

    # Ø§Ø®ØªÙŠØ§Ø± Ø·Ø±ÙŠÙ‚Ø© pooling
    if input.pooling == "cls":
        embedding_768 = last_hidden_state[:, 0, :]  # Ø£ÙˆÙ„ ØªÙˆÙƒÙ†
    else:
        # mean pooling Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© attention mask
        mask = attention_mask[..., None]
        sum_embeddings = np.sum(last_hidden_state * mask, axis=1)
        sum_mask = np.clip(mask.sum(axis=1), a_min=1e-9, a_max=None)
        embedding_768 = sum_embeddings / sum_mask

    # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù
    reduce_dim = min(input.reduce_dim, projection_matrix.shape[1])
    projection_sub = projection_matrix[:, :reduce_dim]
    embedding_reduced = embedding_768 @ projection_sub

    # ØªØ·Ø¨ÙŠØ¹ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
    if input.normalize:
        norms = np.linalg.norm(embedding_reduced, axis=1, keepdims=True)
        embedding_reduced = embedding_reduced / np.clip(norms, 1e-9, None)

    return {
        "embedding": embedding_reduced[0].tolist(),
        "shape": list(embedding_reduced.shape),
        "options": {
            "normalize": input.normalize,
            "reduce_dim": reduce_dim,
            "pooling": input.pooling,
        },
    }