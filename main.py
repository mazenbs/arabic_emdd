from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import onnxruntime as ort
from transformers import AlbertTokenizer
import numpy as np
import re

app = FastAPI(title="Arabic ALBERT Embedding API")

# ==============================
# ØªÙØ¹ÙŠÙ„ CORS
# ==============================
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==============================
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# ==============================
TOKENIZER_PATH = "models/asafaya/albert-base-arabic"
MODEL_PATH = "models/albert_arabic_wa_merged.onnx"
TARGET_DIM = 384
CHUNK_SIZE = 50
SENTENCE_SPLIT_REGEX = r'(?<=[.!ØŸ])\s+'

# ==============================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# ==============================
tokenizer = AlbertTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)
session = ort.InferenceSession(MODEL_PATH, providers=["CPUExecutionProvider"])

# ==============================
# Ù…ØµÙÙˆÙØ© Ø¥Ø³Ù‚Ø§Ø· Ù…Ø­Ø³Ù‘Ù†Ø©
# ==============================
np.random.seed(42)
projection_matrix = np.random.normal(0, 0.1, (768, TARGET_DIM)).astype(np.float32)

# ==============================
# Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==============================
class TextInput(BaseModel):
    text: str
    normalize: bool = True
    return_dim: int = TARGET_DIM
    mean_pooling: bool = True

# ==============================
# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø¬Ù…Ù„
# ==============================
def split_text_to_sentences(text):
    sentences = re.split(SENTENCE_SPLIT_REGEX, text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences

# ==============================
# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„Ø© Ø¥Ù„Ù‰ chunks ØµØºÙŠØ±Ø©
# ==============================
def chunk_text(text, chunk_size=CHUNK_SIZE):
    words = text.split()
    chunks = [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# ==============================
# Ø­Ø³Ø§Ø¨ embedding Ù„ÙƒÙ„ chunk
# ==============================
def compute_embedding(text, mean_pooling=True, normalize=True, return_dim=TARGET_DIM):
    inputs = tokenizer(text, return_tensors="np", truncation=True, max_length=128)
    outputs = session.run(None, {"input_ids": inputs["input_ids"], "attention_mask": inputs["attention_mask"]})
    last_hidden = outputs[0]

    if mean_pooling:
        embedding = last_hidden.mean(axis=1)
    else:
        embedding = last_hidden[:, 0, :]

    embedding_projected = embedding @ projection_matrix[:, :return_dim]

    if normalize:
        norm = np.linalg.norm(embedding_projected, axis=1, keepdims=True)
        embedding_projected = embedding_projected / (norm + 1e-10)

    return embedding_projected[0]

# ==============================
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ embedding Ù…Ø¹ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„ Ùˆ chunks
# ==============================
def text_to_embedding(text, mean_pooling=True, normalize=True, return_dim=TARGET_DIM):
    if not text.strip():
        raise ValueError("Ø§Ù„Ù†Øµ ÙØ§Ø±Øº")

    sentences = split_text_to_sentences(text)
    all_chunk_embeddings = []

    for sentence in sentences:
        chunks = chunk_text(sentence, CHUNK_SIZE)
        chunk_embeddings = [compute_embedding(chunk, mean_pooling, normalize, return_dim) for chunk in chunks]
        all_chunk_embeddings.extend(chunk_embeddings)

    final_embedding = np.mean(np.stack(all_chunk_embeddings), axis=0)
    if normalize:
        final_embedding /= (np.linalg.norm(final_embedding) + 1e-10)

    return final_embedding, sentences

# ==============================
# POST /embed
# ==============================
@app.post("/embed")
def embed_text(data: TextInput):
    try:
        final_embedding, sentences = text_to_embedding(
            data.text, data.mean_pooling, data.normalize, data.return_dim
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    return {
        "num_chunks": len(sentences),
        "chunks": sentences,
        "embedding": final_embedding.tolist()
    }

# ==============================
# GET /embed
# ==============================
@app.get("/embed")
def embed_text_get(
    text: str = Query(..., description="Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡"),
    normalize: bool = Query(True),
    mean_pooling: bool = Query(True),
    return_dim: int = Query(TARGET_DIM),
):
    try:
        final_embedding, sentences = text_to_embedding(
            text, mean_pooling, normalize, return_dim
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    return {
        "num_chunks": len(sentences),
        "chunks": sentences,
        "embedding": final_embedding.tolist()
    }

# ==============================
# Health check
# ==============================
@app.get("/health")
def health():
    return {
        "status": "ok",
        "model": MODEL_PATH.split("/")[-1],
        "tokenizer": TOKENIZER_PATH.split("/")[-1],
    }

@app.get("/")
def home():
    return {"message": "Arabic ALBERT Embedding API is running ğŸš€"}
