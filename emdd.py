import onnxruntime as ort
from transformers import AlbertTokenizer
import numpy as np

# ==============================
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# ==============================
TOKENIZER_PATH = "models/asafaya/albert-base-arabic"
MODEL_PATH = "models/albert_arabic_wa_merged.onnx"
TARGET_DIM = 384
CHUNK_SIZE = 150  # Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ chunk

# ==============================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­ÙˆÙ„
# ==============================
tokenizer = AlbertTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)

session_options = ort.SessionOptions()
session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
session = ort.InferenceSession(MODEL_PATH, sess_options=session_options, providers=["CPUExecutionProvider"])

# ==============================
# Ù…ØµÙÙˆÙØ© Ø¥Ø³Ù‚Ø§Ø· Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
# ==============================
np.random.seed(42)
projection_matrix = np.random.normal(0, 0.1, (768, TARGET_DIM)).astype(np.float32)

# ==============================
# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ (chunks)
# ==============================
def chunk_text(text, chunk_size=CHUNK_SIZE):
    words = text.split()
    chunks = [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# ==============================
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡ (Embedding)
# ==============================
def text_to_vector(
    text: str,
    mean_pooling: bool = True,
    normalize: bool = True,
    return_dim: int = TARGET_DIM
):
    if not text.strip():
        raise ValueError("âŒ Ø§Ù„Ù†Øµ ÙØ§Ø±Øº")

    chunks = chunk_text(text)
    all_embeddings = []

    for chunk in chunks:
        inputs = tokenizer(chunk, return_tensors="np", truncation=True, max_length=128)
        ort_inputs = {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
        }
        if "token_type_ids" in inputs:
            ort_inputs["token_type_ids"] = inputs["token_type_ids"]

        outputs = session.run(None, ort_inputs)
        last_hidden = outputs[0]

        if mean_pooling:
            embedding = last_hidden.mean(axis=1)
        else:
            embedding = last_hidden[:, 0, :]  # CLS token

        # Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        embedding_projected = embedding @ projection_matrix[:, :return_dim]

        # Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        if normalize:
            norm = np.linalg.norm(embedding_projected, axis=1, keepdims=True)
            embedding_projected = embedding_projected / (norm + 1e-10)

        all_embeddings.append(embedding_projected[0])

    # Ø¯Ù…Ø¬ ÙƒÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠ Ù…ØªØ¬Ù‡ ÙˆØ§Ø­Ø¯
    final_embedding = np.mean(np.stack(all_embeddings), axis=0)
    if normalize:
        final_embedding /= (np.linalg.norm(final_embedding) + 1e-10)

    return final_embedding

# ==============================
# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
# ==============================
if __name__ == "__main__":
    text = "Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙŠÙ…Ù†ÙŠ Ù…Ù† Ø£ÙØ¶Ù„ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    embedding = text_to_vector(text)
    print("âœ… Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯:", len(embedding))
    print("ğŸ“Š Ø£ÙˆÙ„ 10 Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„Ù…ØªØ¬Ù‡:", embedding[:10])
